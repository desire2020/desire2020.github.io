---
layout:   research
title:    NeurIPS 2022&#58; InsNet&#58; An Efficient, Flexible, and Performant Insertion-based Text Generation Model
category: publications
---

## Abstract
We propose InsNet, an expressive insertion-based text generator with efficient training and flexible decoding (parallel or sequential). Unlike most existing insertion-based text generation works that require re-encoding of the (decoding) context after each insertion operation and thus are inefficient to train, InsNet only requires one pass of context encoding for the entire insertion sequence during training by using a novel insertion-oriented position encoding to enable computation sharing. Furthermore, InsNet provides a controllable switch between parallel and sequential decoding, making it flexible to handle more parallelizable tasks such as machine translation to support efficient decoding, or less parallelizable tasks such as lexically constrained text generation to guarantee high-quality outputs. Experiments on two unsupervised lexically constrained text generation datasets and three machine translation datasets demonstrate InsNetâ€™s advantages over previous insertion-based methods in terms of training speed, inference efficiency, and generation quality.


## Paper Link
<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/2e32d3a10985fc94c7e11ee6ea165cca-Abstract-Conference.html">https://proceedings.neurips.cc/paper_files/paper/2022/hash/2e32d3a10985fc94c7e11ee6ea165cca-Abstract-Conference.html</a>

## Status

Accepted (Poster)

